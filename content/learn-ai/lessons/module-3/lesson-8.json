{
  "moduleId": 3,
  "lessonId": 8,
  "slug": "dimensionality-reduction",
  "title": "Dimensionality Reduction",
  "isFree": false,
  "content": "<h2>Module 3, Lesson 8: Dimensionality Reduction</h2>\n<h3>The Hook</h3>\n<p>High-dimensional data is the norm in modern data science. The curse of dimensionality makes models harder to train. Dimensionality reduction compresses data while preserving information.</p>\n<h3>The Concept</h3>\n<h4>1. The curse of dimensionality</h4>\n<p>Understanding The curse of dimensionality is foundational to this topic. Indian companies and research institutions apply these principles in production AI systems every day.</p>\n<h4>2. Principal Component Analysis (PCA)</h4>\n<p>Principal Component Analysis (PCA) forms the core methodology for practitioners. India's tech ecosystem from Bengaluru to Hyderabad deploys these approaches at scale.</p>\n<h4>3. t-SNE and UMAP for visualisation</h4>\n<p>t-SNE and UMAP for visualisation is critical for building reliable AI solutions. Top Indian firms like TCS, Wipro, Infosys, and startups like Razorpay all leverage these techniques.</p>\n<h4>4. Autoencoders for non-linear reduction</h4>\n<p>Mastering Autoencoders for non-linear reduction separates good practitioners from great ones. Apply these concepts on Indian open datasets from data.gov.in to build hands-on expertise.</p>\n<h3>The Illustration</h3>\n<p>This lesson covered The curse of dimensionality, Principal Component Analysis (PCA), t-SNE and UMAP for visualisation, and Autoencoders for non-linear reduction. These are the building blocks of dimensionality reduction — master them through practice on real Indian datasets.</p>",
  "quiz": [
    {
      "question": "What is the curse of dimensionality?",
      "options": [
        "Too much training data",
        "Difficulty training models when features far outnumber samples — data becomes sparse",
        "Models running too slowly",
        "Too many target classes"
      ],
      "correct_answer": 1
    },
    {
      "question": "What does PCA stand for?",
      "options": [
        "Primary Cluster Analysis",
        "Principal Component Analysis",
        "Probabilistic Classification Algorithm",
        "Predictive Component Approximation"
      ],
      "correct_answer": 1
    },
    {
      "question": "Which technique best visualises high-dimensional cluster structure in 2D?",
      "options": [
        "PCA",
        "Linear Regression",
        "t-SNE or UMAP",
        "Decision Trees"
      ],
      "correct_answer": 2
    },
    {
      "question": "What is the bottleneck layer in an autoencoder?",
      "options": [
        "A performance limitation",
        "The compressed representation layer",
        "A security layer",
        "The output layer"
      ],
      "correct_answer": 1
    },
    {
      "question": "How does PCA reduce dimensions?",
      "options": [
        "Removing low-variance features",
        "Projecting data onto directions of maximum variance",
        "Selecting most correlated features",
        "Clustering similar features"
      ],
      "correct_answer": 1
    }
  ]
}
