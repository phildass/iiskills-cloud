{
  "moduleId": 4,
  "lessonId": 7,
  "slug": "pytorch-basics",
  "title": "PyTorch Basics",
  "isFree": false,
  "content": "<h2>Module 4, Lesson 7: PyTorch Basics</h2>\n<h3>The Hook</h3>\n<p>PyTorch is Facebook's deep learning framework, favoured in research for its dynamic computation graphs and Pythonic API. In India, IIT Delhi, IISc, and Indian AI research labs predominantly use PyTorch for their deep learning research and experiments.</p>\n<p>This lesson covers the essential concepts, practical tools, and India-specific applications you need to master pytorch basics.</p>\n<h3>The Concept</h3>\n<h4>1. Tensors and Autograd: Automatic Differentiation</h4>\n<p>Understanding tensors and autograd: automatic differentiation is foundational to pytorch basics. Practitioners apply these principles daily in building robust AI systems for India's growing tech ecosystem.</p>\n<h4>2. Dynamic Computation Graphs</h4>\n<p>Dynamic Computation Graphs provides the theoretical and practical framework for implementing effective solutions. India's technology sector widely adopts these approaches in production systems.</p>\n<h4>3. Building Models with torch.nn.Module</h4>\n<p>The application of building models with torch.nn.module is critical for production-grade AI systems. Leading Indian companies rely on these techniques to build scalable, reliable AI products.</p>\n<h4>4. Training Loop: Forward, Loss, Backward, Step</h4>\n<p>Training Loop: Forward, Loss, Backward, Step represents advanced practice in pytorch basics. Mastering this concept is what distinguishes expert AI practitioners building cutting-edge systems.</p>\n<h3>The Illustration</h3>\n<p>In this lesson you covered Tensors and Autograd: Automatic Differentiation, Dynamic Computation Graphs, Building Models with torch.nn.Module, and Training Loop: Forward, Loss, Backward, Step â€” all essential aspects of pytorch basics. Apply these concepts in your projects using India-specific datasets to build real-world experience and a strong portfolio.</p>",
  "quiz": [
    {
      "question": "What makes PyTorch's computation graph \"dynamic\"?",
      "options": [
        "It uses faster GPU algorithms",
        "It builds the graph at runtime during forward pass, enabling flexible architectures",
        "It automatically chooses the best architecture",
        "It dynamically allocates GPU memory"
      ],
      "correct_answer": 1
    },
    {
      "question": "What does autograd do in PyTorch?",
      "options": [
        "Automatically builds neural network architectures",
        "Automatically computes gradients for backpropagation",
        "Automatically downloads datasets",
        "Automatically tunes hyperparameters"
      ],
      "correct_answer": 1
    },
    {
      "question": "What is the base class for all PyTorch neural network modules?",
      "options": [
        "torch.Layer",
        "torch.Model",
        "torch.nn.Module",
        "torch.network.Base"
      ],
      "correct_answer": 2
    },
    {
      "question": "What does optimizer.zero_grad() do in the training loop?",
      "options": [
        "Resets the model weights",
        "Clears accumulated gradients from the previous step",
        "Sets the learning rate to zero",
        "Initialises model parameters"
      ],
      "correct_answer": 1
    },
    {
      "question": "Which Python decorator disables gradient computation during inference?",
      "options": [
        "@torch.inference",
        "@torch.no_grad()",
        "@torch.eval_mode",
        "@no_gradients"
      ],
      "correct_answer": 1
    }
  ]
}
